{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Applying inner evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Requirements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gather the required libraries, classes and function for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyImageML` is a Python package that has been developed under this project, which has several utils for plotting images and extracting features from them, features that later could be used along with Machine Learning algorithms to solve typical ML tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, r\"C:\\Users\\fscielzo\\Documents\\Packages\\PyImageML_Package_Private\")\n",
    "from PyImageML.preprocessing import ImageFeaturesExtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyMachineLearning` is another custom Python package that contains efficient utils to be used in real Machine Learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, r'C:\\Users\\fscielzo\\Documents\\Packages\\PyMachineLearning_Package_Private')\n",
    "from PyMachineLearning.evaluation import SimpleEvaluation\n",
    "from PyMachineLearning.preprocessing import scaler, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to read an process the data a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `files_list.txt` is a txt file with two 'columns', the first one with the old paths of the images of the data-set, and the second with the images class (neutral (0) or fire (1)).\n",
    "\n",
    "-  We read `files_list.txt as a data-frame.\n",
    "\n",
    "- We extract the names of the images files.\n",
    "\n",
    "- We build a list with the new path of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the names of the images files as well as their class/category.\n",
    "files_list_name = r'C:\\Users\\fscielzo\\Documents\\DataScience-GitHub\\Image Analysis\\Image-Classification\\Fire-Detection\\files_list.txt'\n",
    "files_df = pl.read_csv(files_list_name, separator='\\t', has_header=False, new_columns=['path', 'class'])\n",
    "img_files_names = [files_df['path'][i].split('/')[1] for i in range(len(files_df))]\n",
    "\n",
    "# building a list with the current paths of the data-set images.\n",
    "img_path_list = []\n",
    "folder_path = r'C:\\Users\\fscielzo\\Documents\\DataScience-GitHub\\Image Analysis\\Image-Classification\\Fire-Detection\\Data'\n",
    "for filename in img_files_names:\n",
    "    img_path_list.append(os.path.join(folder_path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining Response and Predictors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the response and predictors.\n",
    "\n",
    "- **Predictors:** a list with the paths of the images files.\n",
    "- **Response:** a vector (1D array) that identify the category of each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = files_df['class'].to_numpy()\n",
    "X = img_path_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining the outer validation method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the validation method to be used in the outer evaluation.\n",
    "\n",
    "The outer evaluation consist of estimating the future performance of a ML model, that is, measure how the model will work predicting new data.  This evaluation is usually done only with the best model (or pipeline) according to the inner evaluation.\n",
    "\n",
    "The validation method is the procedure used to estimate that performance, and in this case train-test split (also known as hold-out or simple validation) will be used.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train-Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly divided  the whole predictor matrix `X` and response `Y` into two sets, a training set (75%) and a test one (25%).\n",
    "\n",
    "The training will be used for the training phase, also known as inner evaluation, the test in the testing one, also known as outer evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=123, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_121.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_131.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_194.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_34.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_92.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_93.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_248.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_270.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_3.jpg',\n",
       " 'C:\\\\Users\\\\fscielzo\\\\Documents\\\\DataScience-GitHub\\\\Image Analysis\\\\Image-Classification\\\\Fire-Detection\\\\Data\\\\image_18.jpg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining the inner validation method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the validation method for the inner evaluation. \n",
    "\n",
    "The inner evaluation consist on compare different ML alternatives (also known as pipelines) and select the best one base on their predictive performance. All this procedure is part of the training phase, so that it is done using the training set.\n",
    "\n",
    "Once the best pipeline is selected, outer evaluation applied to it, for estimating its future performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we are going to use KFold Cross Validations as validation method for the inner evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **KFold Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an stratified KFold CV with 4 folds and random shuffled, since it is much more precise than simple validation (hold-out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = StratifiedKFold(n_splits=4, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining the pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define the ML pipelines that will be tested along the project.\n",
    "\n",
    "The pipelines are a combination of preprocessing steps (transformers) plus a model (estimator), and are applied in a sequential way, form the first step to the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here some parameters to use along with ImageFeaturesExtraction are defined:\n",
    "\n",
    "CELLS_PER_BLOCK_HOR = 2\n",
    "CELLS_PER_BLOCK_VER = 2\n",
    "PIXELS_PER_CELL_HOR = 8\n",
    "PIXELS_PER_CELL_VER = 8\n",
    "orientations = 8\n",
    "pixels_per_cell=(PIXELS_PER_CELL_HOR, PIXELS_PER_CELL_VER)\n",
    "cells_per_block=(CELLS_PER_BLOCK_HOR, CELLS_PER_BLOCK_VER)\n",
    "\n",
    "mask_3 = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "\n",
    "img_height = 240\n",
    "img_width = 184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {} \n",
    "\n",
    "models = {'knn': KNeighborsClassifier(n_jobs=-1), \n",
    "          'trees': DecisionTreeClassifier(random_state=123), \n",
    "          'extra_trees': ExtraTreesClassifier(random_state=123),\n",
    "          'RF': RandomForestClassifier(random_state=123), \n",
    "          'HGB': HistGradientBoostingClassifier(random_state=123), \n",
    "          'MLP': MLPClassifier(random_state=123),\n",
    "          'LinearSVM': LinearSVC(random_state=123),  \n",
    "          'XGB': XGBClassifier(random_state=123),\n",
    "          'Logistic': LogisticRegression(max_iter=250, solver='saga', random_state=123),\n",
    "          'LGBM': LGBMClassifier(random_state=123, verbose=-1),\n",
    "          'SVM': SVC(random_state=123)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    \n",
    "    pipelines[model_name] = Pipeline([\n",
    "                ('feature_extraction', ImageFeaturesExtraction(method='pixels', image_height=img_height, image_width=img_width, convert_to_gray=True, \n",
    "                                                               filter='equalized', weights=mask_3, format='array', orientations=orientations, pixels_per_cell=pixels_per_cell, \n",
    "                                                               cells_per_block=cells_per_block, transform_sqrt=True, reshape=False, statistics=None, n_clusters=100)),\n",
    "                ('scaler', scaler(apply=False, method='standard')),\n",
    "                ('pca', pca(apply=False, n_components=5, random_state=123)),\n",
    "                (model_name, model) \n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applying inner evaluation with pixels features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section inner evaluation will be applied considering only the pixels method for features extraction.\n",
    "\n",
    "We could do a more general inner evaluation considering the feature extraction method as a hyper-parameter to be optimize, but instead we prefer to apply a more exhaustive hyper-parameter search over each specific features extraction method between the three addressed in this project, since we consider it as a more proper way for achieving a better model, as well as for understanding how different parameters and alternatives work with each feature extraction method, so that we will be able to asses more precisely how those parameters affect to de features extraction method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grids for HPO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible general pipeline for the preprocessing part. \n",
    "\n",
    "If we would have followed a more general HPO where the features extraction method were another alternative to explore, instead of being fixed, this grid could have been used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef preprocessing_param_grid(trial):\\n\\n    # Fixed Grid\\n    param_grid = {\\n        'feature_extraction__method': trial.suggest_categorical('feature_extraction__method', ['pixels', 'HOG', 'CNN']),\\n        'scaler__apply': trial.suggest_categorical('scaler__apply', [True, False]),\\n        'pca__apply': trial.suggest_categorical('pca__apply', [True, False])\\n    }\\n\\n    # Conditioned Grid\\n     \\n    ################################################################################################################ \\n    if param_grid['scaler__apply'] == True:\\n\\n        param_grid.update({'scaler__method': trial.suggest_categorical('scaler__method', ['standard', 'min-max'])})\\n\\n    ################################################################################################################\\n    if param_grid['pca__apply'] == True:\\n\\n        param_grid.update({'pca__n_components': trial.suggest_int('pca__n_components', 2, 80)})\\n\\n    ################################################################################################################\\n    if param_grid['feature_extraction__method'] != 'CNN': # We CNN filters are not allowed in our implementations (at least yet).\\n\\n        param_grid.update({'feature_extraction__filter': trial.suggest_categorical('feature_extraction__filter', [None, 'equalized', 'sobel', 'canny'])})\\n\\n    ################################################################################################################\\n    if param_grid['feature_extraction__method'] == 'pixels':\\n\\n        param_grid.update({'feature_extraction__convert_to_gray': trial.suggest_categorical('feature_extraction__convert_to_gray', [True, False])})\\n   \\n    ################################################################################################################\\n    if param_grid['feature_extraction__method'] == 'HOG':\\n\\n        if param_grid['feature_extraction__reshape'] == False:\\n\\n            param_grid.update({'feature_extraction__statistics': trial.suggest_categorical('feature_extraction__statistics', ['BVW', \\n                                                                                                                            'mean', 'mean-std',\\n                                                                                                                            'mean-median-std',\\n                                                                                                                            'mean-Q25-median-Q75-std'])})\\n\\n            if param_grid['feature_extraction__statistics'] == 'BVW':\\n\\n                param_grid.update({'feature_extraction__n_clusters': trial.suggest_int('feature_extraction__n_clusters', 2, 100)})\\n\\n        else:\\n\\n            param_grid.update({'pca__apply': trial.suggest_categorical('pca__apply', [True])})\\n\\n    return param_grid\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def preprocessing_param_grid(trial):\n",
    "\n",
    "    # Fixed Grid\n",
    "    param_grid = {\n",
    "        'feature_extraction__method': trial.suggest_categorical('feature_extraction__method', ['pixels', 'HOG', 'CNN']),\n",
    "        'scaler__apply': trial.suggest_categorical('scaler__apply', [True, False]),\n",
    "        'pca__apply': trial.suggest_categorical('pca__apply', [True, False])\n",
    "    }\n",
    "\n",
    "    # Conditioned Grid\n",
    "     \n",
    "    ################################################################################################################ \n",
    "    if param_grid['scaler__apply'] == True:\n",
    "\n",
    "        param_grid.update({'scaler__method': trial.suggest_categorical('scaler__method', ['standard', 'min-max'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['pca__apply'] == True:\n",
    "\n",
    "        param_grid.update({'pca__n_components': trial.suggest_int('pca__n_components', 2, 80)})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] != 'CNN': # We CNN filters are not allowed in our implementations (at least yet).\n",
    "\n",
    "        param_grid.update({'feature_extraction__filter': trial.suggest_categorical('feature_extraction__filter', [None, 'equalized', 'sobel', 'canny'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] == 'pixels':\n",
    "\n",
    "        param_grid.update({'feature_extraction__convert_to_gray': trial.suggest_categorical('feature_extraction__convert_to_gray', [True, False])})\n",
    "   \n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] == 'HOG':\n",
    "\n",
    "        if param_grid['feature_extraction__reshape'] == False:\n",
    "\n",
    "            param_grid.update({'feature_extraction__statistics': trial.suggest_categorical('feature_extraction__statistics', ['BVW', \n",
    "                                                                                                                            'mean', 'mean-std',\n",
    "                                                                                                                            'mean-median-std',\n",
    "                                                                                                                            'mean-Q25-median-Q75-std'])})\n",
    "\n",
    "            if param_grid['feature_extraction__statistics'] == 'BVW':\n",
    "\n",
    "                param_grid.update({'feature_extraction__n_clusters': trial.suggest_int('feature_extraction__n_clusters', 2, 100)})\n",
    "\n",
    "        else:\n",
    "\n",
    "            param_grid.update({'pca__apply': trial.suggest_categorical('pca__apply', [True])})\n",
    "\n",
    "    return param_grid\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the preprocessing grid for applying hyper-parameter optimization (HPO) fixing the feature extraction method to pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pixels_param_grid(trial):\n",
    "\n",
    "    # Fixed Grid\n",
    "    param_grid = {\n",
    "        'feature_extraction__method': trial.suggest_categorical('feature_extraction__method', ['pixels']),\n",
    "        'scaler__apply': trial.suggest_categorical('scaler__apply', [True, False]),\n",
    "        'pca__apply': trial.suggest_categorical('pca__apply', [True]) # To avoid high-dimensionality on p (num. features)\n",
    "    }\n",
    "\n",
    "    # Conditioned Grid\n",
    "     \n",
    "    ################################################################################################################ \n",
    "    if param_grid['scaler__apply'] == True:\n",
    "\n",
    "        param_grid.update({'scaler__method': trial.suggest_categorical('scaler__method', ['standard', 'min-max'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['pca__apply'] == True:\n",
    "\n",
    "        param_grid.update({'pca__n_components': trial.suggest_int('pca__n_components', 2, 150)})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] != 'CNN': # We CNN filters are not allowed in our implementations (at least yet).\n",
    "\n",
    "        param_grid.update({'feature_extraction__filter': trial.suggest_categorical('feature_extraction__filter', [None, 'equalized', 'sobel', 'canny',\n",
    "                                                                                                                  'convolve', 'hessian', 'prewitt'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] == 'pixels':\n",
    "\n",
    "        param_grid.update({'feature_extraction__convert_to_gray': trial.suggest_categorical('feature_extraction__convert_to_gray', [True, False])})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining grids for Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_knn_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'knn__n_neighbors': trial.suggest_int('knn__n_neighbors', 1, 25),\n",
    "        'knn__metric': trial.suggest_categorical('knn__metric', ['cosine', 'minkowski', 'cityblock'])\n",
    "    })\n",
    "\n",
    "    if param_grid['knn__metric'] == 'minkowski':\n",
    "        param_grid['knn__p'] = trial.suggest_int('knn__p', 1, 4)\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_trees_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'trees__max_depth': trial.suggest_categorical('trees__max_depth', [None, 2, 5, 7, 10, 20, 30]),\n",
    "        'trees__min_samples_split': trial.suggest_int('trees__min_samples_split', 2, 25),\n",
    "        'trees__min_samples_leaf': trial.suggest_int('trees__min_samples_leaf', 2, 25),\n",
    "        'trees__splitter': trial.suggest_categorical('trees__splitter', ['best', 'random']),\n",
    "        'trees__criterion': trial.suggest_categorical('trees__criterion', ['log_loss', 'gini', 'entropy'])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_extra_trees_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'extra_trees__n_estimators': trial.suggest_categorical('extra_trees__n_estimators', [30, 50, 75, 100, 120]),\n",
    "        'extra_trees__max_depth': trial.suggest_categorical('extra_trees__max_depth', [3, 5, 7, 10, 20, 30]),\n",
    "        'extra_trees__min_samples_split': trial.suggest_int('extra_trees__min_samples_split', 2, 20),\n",
    "        'extra_trees__min_samples_leaf': trial.suggest_int('extra_trees__min_samples_leaf', 2, 20),\n",
    "        'extra_trees__criterion': trial.suggest_categorical('extra_trees__criterion', ['gini']),\n",
    "        'extra_trees__max_features': trial.suggest_categorical('extra_trees__max_features', [0.7, 0.8, 0.9, 1.0])\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_HGB_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'HGB__max_depth': trial.suggest_categorical('HGB__max_depth', [5, 10, 20, 30, 40, 50]),\n",
    "        'HGB__l2_regularization': trial.suggest_float('HGB__l2_regularization', 0.01, 0.7, log=True),\n",
    "        'HGB__max_iter': trial.suggest_categorical('HGB__max_iter', [50, 70, 100, 130, 150])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_XGB_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'XGB__max_depth': trial.suggest_categorical('XGB__max_depth', [10, 20, 30, 40, 50, 70, 100]),\n",
    "        'XGB__reg_lambda': trial.suggest_float('XGB__reg_lambda', 0, 1, step=0.05, log=False),\n",
    "        'XGB__n_estimators': trial.suggest_categorical('XGB__n_estimators', [50, 70, 100, 130, 150]),\n",
    "        'XGB__eta': trial.suggest_float('XGB__eta', 0, 0.3, step=0.02, log=False),\n",
    "        'XGB__alpha': trial.suggest_float('XGB__alpha', 0.2, 1, step=0.01, log=False)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_RF_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'RF__n_estimators': trial.suggest_categorical('RF__n_estimators', [30, 50, 75, 100, 120, 150, 200, 250]),\n",
    "        'RF__max_depth': trial.suggest_categorical('RF__max_depth', [3, 4, 5, 7, 10, 20, 30]),\n",
    "        'RF__min_samples_split': trial.suggest_int('RF__min_samples_split', 2, 20),\n",
    "        'RF__min_samples_leaf': trial.suggest_int('RF__min_samples_leaf', 2, 20),\n",
    "        'RF__criterion': trial.suggest_categorical('RF__criterion', ['gini', 'entropy']),\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_linear_SVM_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LinearSVM__C': trial.suggest_float('SVM__C', 0.001, 2, log=True),\n",
    "        'LinearSVM__class_weight': trial.suggest_categorical('LinearSVM__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_MLP_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'MLP__learning_rate_init': trial.suggest_float('MLP__learning_rate_init', 0.0001, 0.2, log=True),\n",
    "        'MLP__alpha': trial.suggest_float('MLP__alpha', 0.01, 1, log=True)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_logistic_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'Logistic__penalty':  trial.suggest_categorical('Logistic__penalty', ['l1', 'l2', 'elasticnet', None]),\n",
    "        'Logistic__C': trial.suggest_float('Logistic__C', 0.001, 2, log=True),\n",
    "        'Logistic__class_weight': trial.suggest_categorical('Logistic__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    if param_grid['Logistic__penalty'] == 'elasticnet':\n",
    "        param_grid.update({'Logistic__l1_ratio': trial.suggest_float('Logistic__l1_ratio', 0.1, 1, log=True)})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_SVM_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'SVM__C': trial.suggest_float('SVM__C', 0.1, 5, log=True),\n",
    "        'SVM__kernel': trial.suggest_categorical('SVM__kernel', ['poly', 'rbf', 'sigmoid']),\n",
    "    })\n",
    "\n",
    "    if param_grid['SVM__kernel'] == 'poly':\n",
    "\n",
    "        param_grid.update({\n",
    "            'SVM__degree': trial.suggest_int('SVM__degree', 1, 5)\n",
    "        })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_LGBM_pixels(trial):\n",
    "\n",
    "    param_grid = preprocessing_pixels_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LGBM__max_depth': trial.suggest_int('LGBM__max_depth', 2, 200),\n",
    "        'LGBM__num_leaves': trial.suggest_int('LGBM__num_leaves', 2, 200),\n",
    "        'LGBM__n_estimators': trial.suggest_categorical('LGBM__n_estimators', [30, 50, 70, 100, 120, 150, 180, 200, 250, 300]),\n",
    "        'LGBM__learning_rate': trial.suggest_float('LGBM__learning_rate', 0.0001, 0.1, log=True),\n",
    "        'LGBM__lambda_l1': trial.suggest_float('LGBM__lambda_l1', 0.001, 1, log=True),\n",
    "        'LGBM__lambda_l2': trial.suggest_float('LGBM__lambda_l2', 0.001, 1, log=True),\n",
    "        'LGBM__min_split_gain': trial.suggest_float('LGBM__min_split_gain', 0.001, 0.01, log=True),\n",
    "        'LGBM__min_child_weight': trial.suggest_int('LGBM__min_child_weight', 5, 60),\n",
    "        'LGBM__feature_fraction': trial.suggest_float('LGBM__feature_fraction', 0.1, 0.9, step=0.05)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyper-parameter Optimization (HPO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_score, best_params, inner_results = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'knn'\n",
    "dict_name = 'knn-pixels'\n",
    "param_grid = param_grid_knn_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'trees'\n",
    "dict_name = 'trees-pixels'\n",
    "param_grid = param_grid_trees_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'extra_trees'\n",
    "dict_name = 'extra-trees-pixels'\n",
    "param_grid = param_grid_extra_trees_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HGB'\n",
    "dict_name = 'HGB-pixels'\n",
    "param_grid = param_grid_HGB_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'\n",
    "dict_name = 'RF-pixels'\n",
    "param_grid = param_grid_RF_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'XGB'\n",
    "dict_name = 'XGB-pixels'\n",
    "param_grid = param_grid_XGB_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Logistic'\n",
    "dict_name = 'Logistic-pixels'\n",
    "param_grid = param_grid_logistic_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LinearSVM'\n",
    "dict_name = 'LinearSVM-pixels'\n",
    "param_grid = param_grid_linear_SVM_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SVM'\n",
    "dict_name = 'SVM-pixels'\n",
    "param_grid = param_grid_SVM_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LGBM'\n",
    "dict_name = 'LGBM-pixels'\n",
    "param_grid = param_grid_LGBM_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=40, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LGBM'\n",
    "dict_name = 'LGBM-pixels'\n",
    "param_grid = param_grid_LGBM_pixels\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=40, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applying inner evaluation with HOG features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section inner evaluation will be applied considering only the HOG method for features extraction.\n",
    "\n",
    "We could do a more general inner evaluation considering the feature extraction method as a hyper-parameter to be optimize, but instead we prefer to apply a more exhaustive hyper-parameter search over each specific features extraction method between the three addressed in this project, since we consider it as a more proper way for achieving a better model, as well as for understanding how different parameters and alternatives work with each feature extraction method, so that we will be able to asses more precisely how those parameters affect to de features extraction method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grids for HPO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the preprocessing grid for applying hyper-parameter optimization (HPO) fixing the feature extraction method to **HOG**, but we will divide the exploration in two parts, one assuming a reshaped HOG features vector, and another without reshaping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_HOG_reshaped_param_grid(trial):\n",
    "\n",
    "    # Fixed Grid\n",
    "    param_grid = {\n",
    "        'feature_extraction__method': trial.suggest_categorical('feature_extraction__method', ['HOG']),\n",
    "        'scaler__apply': trial.suggest_categorical('scaler__apply', [True, False]),\n",
    "        'pca__apply': trial.suggest_categorical('pca__apply', [True, False])\n",
    "    }\n",
    "\n",
    "    # Conditioned Grid\n",
    "     \n",
    "    ################################################################################################################ \n",
    "    if param_grid['scaler__apply'] == True:\n",
    "\n",
    "        param_grid.update({'scaler__method': trial.suggest_categorical('scaler__method', ['standard', 'min-max'])})\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "    if param_grid['pca__apply'] == True:\n",
    "\n",
    "        param_grid.update({'pca__n_components': trial.suggest_int('pca__n_components', 2, 25)})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] != 'CNN': # We CNN filters are not allowed in our implementations (at least yet).\n",
    "\n",
    "        param_grid.update({'feature_extraction__filter': trial.suggest_categorical('feature_extraction__filter', [None, 'equalized', 'sobel', 'canny',\n",
    "                                                                                                                  'hessian', 'prewitt'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] == 'HOG':\n",
    "\n",
    "        param_grid.update({'feature_extraction__reshape': trial.suggest_categorical('feature_extraction__reshape', [True])}) # Forcing reshape to be true\n",
    "\n",
    "        param_grid.update({'feature_extraction__statistics': trial.suggest_categorical('feature_extraction__statistics', ['mean', 'mean-std',\n",
    "                                                                                                                          'mean-median-std',\n",
    "                                                                                                                          'mean-Q25-median-Q75-std'])})\n",
    "\n",
    "        if param_grid['feature_extraction__statistics'] == 'BVW':\n",
    "\n",
    "            param_grid.update({'feature_extraction__n_clusters': trial.suggest_int('feature_extraction__n_clusters', 50, 100)})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_HOG_not_reshaped_param_grid(trial):\n",
    "\n",
    "    # Fixed Grid\n",
    "    param_grid = {\n",
    "        'feature_extraction__method': trial.suggest_categorical('feature_extraction__method', ['HOG']),\n",
    "        'scaler__apply': trial.suggest_categorical('scaler__apply', [True, False]),\n",
    "        'pca__apply': trial.suggest_categorical('pca__apply', [True]) # Forcing PCA to combat high dimensionality on p due to not reshaped HGO features\n",
    "    }\n",
    "\n",
    "    # Conditioned Grid\n",
    "     \n",
    "    ################################################################################################################ \n",
    "    if param_grid['scaler__apply'] == True:\n",
    "\n",
    "        param_grid.update({'scaler__method': trial.suggest_categorical('scaler__method', ['standard', 'min-max'])})\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "    if param_grid['pca__apply'] == True:\n",
    "\n",
    "        param_grid.update({'pca__n_components': trial.suggest_int('pca__n_components', 2, 150)})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] != 'CNN': # We CNN filters are not allowed in our implementations (at least yet).\n",
    "\n",
    "        param_grid.update({'feature_extraction__filter': trial.suggest_categorical('feature_extraction__filter', [None, 'equalized', 'sobel', 'canny',\n",
    "                                                                                                                  'hessian', 'prewitt'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['feature_extraction__method'] == 'HOG':\n",
    "\n",
    "        param_grid.update({'feature_extraction__reshape': trial.suggest_categorical('feature_extraction__reshape', [False])}) # Forcing reshape to be False\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining grids for Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_knn_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'knn__n_neighbors': trial.suggest_int('knn__n_neighbors', 1, 25),\n",
    "        'knn__metric': trial.suggest_categorical('knn__metric', ['cosine', 'minkowski', 'cityblock'])\n",
    "    })\n",
    "\n",
    "    if param_grid['knn__metric'] == 'minkowski':\n",
    "        param_grid['knn__p'] = trial.suggest_int('knn__p', 1, 4)\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_knn_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'knn__n_neighbors': trial.suggest_int('knn__n_neighbors', 1, 25),\n",
    "        'knn__metric': trial.suggest_categorical('knn__metric', ['cosine', 'minkowski', 'cityblock'])\n",
    "    })\n",
    "\n",
    "    if param_grid['knn__metric'] == 'minkowski':\n",
    "        param_grid['knn__p'] = trial.suggest_int('knn__p', 1, 4)\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_trees_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'trees__max_depth': trial.suggest_categorical('trees__max_depth', [None, 2, 5, 7, 10, 20, 30]),\n",
    "        'trees__min_samples_split': trial.suggest_int('trees__min_samples_split', 2, 25),\n",
    "        'trees__min_samples_leaf': trial.suggest_int('trees__min_samples_leaf', 2, 25),\n",
    "        'trees__splitter': trial.suggest_categorical('trees__splitter', ['best', 'random']),\n",
    "        'trees__criterion': trial.suggest_categorical('trees__criterion', ['log_loss', 'gini', 'entropy'])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_trees_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'trees__max_depth': trial.suggest_categorical('trees__max_depth', [None, 2, 5, 7, 10, 20, 30]),\n",
    "        'trees__min_samples_split': trial.suggest_int('trees__min_samples_split', 2, 25),\n",
    "        'trees__min_samples_leaf': trial.suggest_int('trees__min_samples_leaf', 2, 25),\n",
    "        'trees__splitter': trial.suggest_categorical('trees__splitter', ['best', 'random']),\n",
    "        'trees__criterion': trial.suggest_categorical('trees__criterion', ['log_loss', 'gini', 'entropy'])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_extra_trees_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'extra_trees__n_estimators': trial.suggest_categorical('extra_trees__n_estimators', [30, 50, 75, 100, 120]),\n",
    "        'extra_trees__max_depth': trial.suggest_categorical('extra_trees__max_depth', [3, 5, 7, 10, 20, 30]),\n",
    "        'extra_trees__min_samples_split': trial.suggest_int('extra_trees__min_samples_split', 2, 20),\n",
    "        'extra_trees__min_samples_leaf': trial.suggest_int('extra_trees__min_samples_leaf', 2, 20),\n",
    "        'extra_trees__criterion': trial.suggest_categorical('extra_trees__criterion', ['gini']),\n",
    "        'extra_trees__max_features': trial.suggest_categorical('extra_trees__max_features', [0.7, 0.8, 0.9, 1.0])\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_extra_trees_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'extra_trees__n_estimators': trial.suggest_categorical('extra_trees__n_estimators', [30, 50, 75, 100, 120]),\n",
    "        'extra_trees__max_depth': trial.suggest_categorical('extra_trees__max_depth', [3, 5, 7, 10, 20, 30]),\n",
    "        'extra_trees__min_samples_split': trial.suggest_int('extra_trees__min_samples_split', 2, 20),\n",
    "        'extra_trees__min_samples_leaf': trial.suggest_int('extra_trees__min_samples_leaf', 2, 20),\n",
    "        'extra_trees__criterion': trial.suggest_categorical('extra_trees__criterion', ['gini']),\n",
    "        'extra_trees__max_features': trial.suggest_categorical('extra_trees__max_features', [0.7, 0.8, 0.9, 1.0])\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_HGB_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'HGB__max_depth': trial.suggest_categorical('HGB__max_depth', [5, 10, 20, 30, 40, 50]),\n",
    "        'HGB__l2_regularization': trial.suggest_float('HGB__l2_regularization', 0.01, 0.7, log=True),\n",
    "        'HGB__max_iter': trial.suggest_categorical('HGB__max_iter', [50, 70, 100, 130, 150])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_HGB_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'HGB__max_depth': trial.suggest_categorical('HGB__max_depth', [5, 10, 20, 30, 40, 50]),\n",
    "        'HGB__l2_regularization': trial.suggest_float('HGB__l2_regularization', 0.01, 0.7, log=True),\n",
    "        'HGB__max_iter': trial.suggest_categorical('HGB__max_iter', [50, 70, 100, 130, 150])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_XGB_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'XGB__max_depth': trial.suggest_categorical('XGB__max_depth', [10, 20, 30, 40, 50, 70, 100]),\n",
    "        'XGB__reg_lambda': trial.suggest_float('XGB__reg_lambda', 0, 1, step=0.05, log=False),\n",
    "        'XGB__n_estimators': trial.suggest_categorical('XGB__n_estimators', [50, 70, 100, 130, 150]),\n",
    "        'XGB__eta': trial.suggest_float('XGB__eta', 0, 0.3, step=0.02, log=False),\n",
    "        'XGB__alpha': trial.suggest_float('XGB__alpha', 0.2, 1, step=0.01, log=False)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_XGB_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'XGB__max_depth': trial.suggest_categorical('XGB__max_depth', [10, 20, 30, 40, 50, 70, 100]),\n",
    "        'XGB__reg_lambda': trial.suggest_float('XGB__reg_lambda', 0, 1, step=0.05, log=False),\n",
    "        'XGB__n_estimators': trial.suggest_categorical('XGB__n_estimators', [50, 70, 100, 130, 150]),\n",
    "        'XGB__eta': trial.suggest_float('XGB__eta', 0, 0.3, step=0.02, log=False),\n",
    "        'XGB__alpha': trial.suggest_float('XGB__alpha', 0.2, 1, step=0.01, log=False)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_RF_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'RF__n_estimators': trial.suggest_categorical('RF__n_estimators', [30, 50, 75, 100, 120, 150, 200, 250]),\n",
    "        'RF__max_depth': trial.suggest_categorical('RF__max_depth', [3, 4, 5, 7, 10, 20, 30]),\n",
    "        'RF__min_samples_split': trial.suggest_int('RF__min_samples_split', 2, 20),\n",
    "        'RF__min_samples_leaf': trial.suggest_int('RF__min_samples_leaf', 2, 20),\n",
    "        'RF__criterion': trial.suggest_categorical('RF__criterion', ['gini', 'entropy']),\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_RF_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'RF__n_estimators': trial.suggest_categorical('RF__n_estimators', [30, 50, 75, 100, 120, 150, 200, 250]),\n",
    "        'RF__max_depth': trial.suggest_categorical('RF__max_depth', [3, 4, 5, 7, 10, 20, 30]),\n",
    "        'RF__min_samples_split': trial.suggest_int('RF__min_samples_split', 2, 20),\n",
    "        'RF__min_samples_leaf': trial.suggest_int('RF__min_samples_leaf', 2, 20),\n",
    "        'RF__criterion': trial.suggest_categorical('RF__criterion', ['gini', 'entropy']),\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_linear_SVM_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LinearSVM__C': trial.suggest_float('SVM__C', 0.001, 2, log=True),\n",
    "        'LinearSVM__class_weight': trial.suggest_categorical('LinearSVM__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_linear_SVM_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LinearSVM__C': trial.suggest_float('SVM__C', 0.001, 2, log=True),\n",
    "        'LinearSVM__class_weight': trial.suggest_categorical('LinearSVM__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_MLP_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'MLP__learning_rate_init': trial.suggest_float('MLP__learning_rate_init', 0.0001, 0.2, log=True),\n",
    "        'MLP__alpha': trial.suggest_float('MLP__alpha', 0.01, 1, log=True)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_MLP_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'MLP__learning_rate_init': trial.suggest_float('MLP__learning_rate_init', 0.0001, 0.2, log=True),\n",
    "        'MLP__alpha': trial.suggest_float('MLP__alpha', 0.01, 1, log=True)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_logistic_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'Logistic__penalty':  trial.suggest_categorical('Logistic__penalty', ['l1', 'l2', 'elasticnet', None]),\n",
    "        'Logistic__C': trial.suggest_float('Logistic__C', 0.001, 2, log=True),\n",
    "        'Logistic__class_weight': trial.suggest_categorical('Logistic__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    if param_grid['Logistic__penalty'] == 'elasticnet':\n",
    "        param_grid.update({'Logistic__l1_ratio': trial.suggest_float('Logistic__l1_ratio', 0.1, 1, log=True)})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_logistic_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'Logistic__penalty':  trial.suggest_categorical('Logistic__penalty', ['l1', 'l2', 'elasticnet', None]),\n",
    "        'Logistic__C': trial.suggest_float('Logistic__C', 0.001, 2, log=True),\n",
    "        'Logistic__class_weight': trial.suggest_categorical('Logistic__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    if param_grid['Logistic__penalty'] == 'elasticnet':\n",
    "        param_grid.update({'Logistic__l1_ratio': trial.suggest_float('Logistic__l1_ratio', 0.1, 1, log=True)})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_SVM_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'SVM__C': trial.suggest_float('SVM__C', 0.1, 5, log=True),\n",
    "        'SVM__kernel': trial.suggest_categorical('SVM__kernel', ['poly', 'rbf', 'sigmoid']),\n",
    "    })\n",
    "\n",
    "    if param_grid['SVM__kernel'] == 'poly':\n",
    "\n",
    "        param_grid.update({\n",
    "            'SVM__degree': trial.suggest_int('SVM__degree', 1, 5)\n",
    "        })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_SVM_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'SVM__C': trial.suggest_float('SVM__C', 0.1, 5, log=True),\n",
    "        'SVM__kernel': trial.suggest_categorical('SVM__kernel', ['poly', 'rbf', 'sigmoid']),\n",
    "    })\n",
    "\n",
    "    if param_grid['SVM__kernel'] == 'poly':\n",
    "\n",
    "        param_grid.update({\n",
    "            'SVM__degree': trial.suggest_int('SVM__degree', 1, 5)\n",
    "        })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_LGBM_HOG_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LGBM__max_depth': trial.suggest_int('LGBM__max_depth', 2, 200),\n",
    "        'LGBM__num_leaves': trial.suggest_int('LGBM__num_leaves', 2, 200),\n",
    "        'LGBM__n_estimators': trial.suggest_categorical('LGBM__n_estimators', [30, 50, 70, 100, 120, 150, 180, 200, 250, 300]),\n",
    "        'LGBM__learning_rate': trial.suggest_float('LGBM__learning_rate', 0.0001, 0.1, log=True),\n",
    "        'LGBM__lambda_l1': trial.suggest_float('LGBM__lambda_l1', 0.001, 1, log=True),\n",
    "        'LGBM__lambda_l2': trial.suggest_float('LGBM__lambda_l2', 0.001, 1, log=True),\n",
    "        'LGBM__min_split_gain': trial.suggest_float('LGBM__min_split_gain', 0.001, 0.01, log=True),\n",
    "        'LGBM__min_child_weight': trial.suggest_int('LGBM__min_child_weight', 5, 60),\n",
    "        'LGBM__feature_fraction': trial.suggest_float('LGBM__feature_fraction', 0.1, 0.9, step=0.05)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_LGBM_HOG_not_reshaped(trial):\n",
    "\n",
    "    param_grid = preprocessing_HOG_not_reshaped_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LGBM__max_depth': trial.suggest_int('LGBM__max_depth', 2, 200),\n",
    "        'LGBM__num_leaves': trial.suggest_int('LGBM__num_leaves', 2, 200),\n",
    "        'LGBM__n_estimators': trial.suggest_categorical('LGBM__n_estimators', [30, 50, 70, 100, 120, 150, 180, 200, 250, 300]),\n",
    "        'LGBM__learning_rate': trial.suggest_float('LGBM__learning_rate', 0.0001, 0.1, log=True),\n",
    "        'LGBM__lambda_l1': trial.suggest_float('LGBM__lambda_l1', 0.001, 1, log=True),\n",
    "        'LGBM__lambda_l2': trial.suggest_float('LGBM__lambda_l2', 0.001, 1, log=True),\n",
    "        'LGBM__min_split_gain': trial.suggest_float('LGBM__min_split_gain', 0.001, 0.01, log=True),\n",
    "        'LGBM__min_child_weight': trial.suggest_int('LGBM__min_child_weight', 5, 60),\n",
    "        'LGBM__feature_fraction': trial.suggest_float('LGBM__feature_fraction', 0.1, 0.9, step=0.05)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyper-parameter Optimization (HPO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'knn'\n",
    "dict_name = 'knn-HOG-reshaped'\n",
    "param_grid = param_grid_knn_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'knn'\n",
    "dict_name = 'knn-HOG-not-reshaped'\n",
    "param_grid = param_grid_knn_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'trees'\n",
    "dict_name = 'trees-HOG-reshaped'\n",
    "param_grid = param_grid_trees_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'trees'\n",
    "dict_name = 'trees-HOG-not-reshaped'\n",
    "param_grid = param_grid_trees_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'extra_trees'\n",
    "dict_name = 'extra-trees-HOG-reshaped'\n",
    "param_grid = param_grid_extra_trees_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=15, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'extra_trees'\n",
    "dict_name = 'extra-trees-HOG-not-reshaped'\n",
    "param_grid = param_grid_extra_trees_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HGB'\n",
    "dict_name = 'HGB-HOG-reshaped'\n",
    "param_grid = param_grid_HGB_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=15, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HGB'\n",
    "dict_name = 'HGB-HOG-not-reshaped'\n",
    "param_grid = param_grid_HGB_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'\n",
    "dict_name = 'RF-HOG-reshaped'\n",
    "param_grid = param_grid_RF_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'\n",
    "dict_name = 'RF-HOG-not-reshaped'\n",
    "param_grid = param_grid_RF_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'XGB'\n",
    "dict_name = 'XGB-HOG-reshaped'\n",
    "param_grid = param_grid_XGB_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'XGB'\n",
    "dict_name = 'XGB-HOG-not-reshaped'\n",
    "param_grid = param_grid_XGB_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Logistic'\n",
    "dict_name = 'Logistic-HOG-reshaped'\n",
    "param_grid = param_grid_logistic_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Logistic'\n",
    "dict_name = 'Logistic-HOG-not-reshaped'\n",
    "param_grid = param_grid_logistic_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=30, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LinearSVM'\n",
    "dict_name = 'LinearSVM-HOG-reshaped'\n",
    "param_grid = param_grid_linear_SVM_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LinearSVM'\n",
    "dict_name = 'LinearSVM-HOG-not-reshaped'\n",
    "param_grid = param_grid_linear_SVM_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SVM'\n",
    "dict_name = 'SVM-HOG-reshaped'\n",
    "param_grid = param_grid_SVM_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SVM'\n",
    "dict_name = 'SVM-HOG-not-reshaped'\n",
    "param_grid = param_grid_SVM_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LGBM'\n",
    "dict_name = 'LGBM-HOG-reshaped'\n",
    "param_grid = param_grid_LGBM_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LGBM'\n",
    "dict_name = 'LGBM-HOG-not-reshaped'\n",
    "param_grid = param_grid_LGBM_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=25, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MLP'\n",
    "dict_name = 'MLP-HOG-reshaped'\n",
    "param_grid = param_grid_MLP_HOG_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MLP'\n",
    "dict_name = 'MLP-HOG-not-reshaped'\n",
    "param_grid = param_grid_MLP_HOG_not_reshaped\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=10, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applying inner evaluation with CNN features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section inner evaluation will be applied considering only the CNN method for features extraction.\n",
    "\n",
    "We could do a more general inner evaluation considering the feature extraction method as a hyper-parameter to be optimize, but instead we prefer to apply a more exhaustive hyper-parameter search over each specific features extraction method between the three addressed in this project, since we consider it as a more proper way for achieving a better model, as well as for understanding how different parameters and alternatives work with each feature extraction method, so that we will be able to asses more precisely how those parameters affect to de features extraction method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grids for HPO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the preprocessing grid for applying hyper-parameter optimization (HPO) fixing the feature extraction method to CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_CNN_param_grid(trial):\n",
    "\n",
    "    # Fixed Grid\n",
    "    param_grid = {\n",
    "        'feature_extraction__method': trial.suggest_categorical('feature_extraction__method', ['CNN']),\n",
    "        'scaler__apply': trial.suggest_categorical('scaler__apply', [True, False]),\n",
    "        'pca__apply': trial.suggest_categorical('pca__apply', [True]) # Forcing PCA to combat high dimensionality on p.\n",
    "    }\n",
    "\n",
    "    # Conditioned Grid\n",
    "     \n",
    "    ################################################################################################################ \n",
    "    if param_grid['scaler__apply'] == True:\n",
    "\n",
    "        param_grid.update({'scaler__method': trial.suggest_categorical('scaler__method', ['standard', 'min-max'])})\n",
    "\n",
    "    ################################################################################################################\n",
    "    if param_grid['pca__apply'] == True:\n",
    "\n",
    "        param_grid.update({'pca__n_components': trial.suggest_int('pca__n_components', 2, 150)})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining grids for Machine Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_knn_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'knn__n_neighbors': trial.suggest_int('knn__n_neighbors', 1, 25),\n",
    "        'knn__metric': trial.suggest_categorical('knn__metric', ['cosine', 'minkowski', 'cityblock'])\n",
    "    })\n",
    "\n",
    "    if param_grid['knn__metric'] == 'minkowski':\n",
    "        param_grid['knn__p'] = trial.suggest_int('knn__p', 1, 4)\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_extra_trees_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'extra_trees__n_estimators': trial.suggest_categorical('extra_trees__n_estimators', [30, 50, 75, 100, 120]),\n",
    "        'extra_trees__max_depth': trial.suggest_categorical('extra_trees__max_depth', [3, 5, 7, 10, 20, 30]),\n",
    "        'extra_trees__min_samples_split': trial.suggest_int('extra_trees__min_samples_split', 2, 20),\n",
    "        'extra_trees__min_samples_leaf': trial.suggest_int('extra_trees__min_samples_leaf', 2, 20),\n",
    "        'extra_trees__criterion': trial.suggest_categorical('extra_trees__criterion', ['gini']),\n",
    "        'extra_trees__max_features': trial.suggest_categorical('extra_trees__max_features', [0.7, 0.8, 0.9, 1.0])\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_XGB_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'XGB__max_depth': trial.suggest_categorical('XGB__max_depth', [10, 20, 30, 40, 50, 70, 100]),\n",
    "        'XGB__reg_lambda': trial.suggest_float('XGB__reg_lambda', 0, 1, step=0.05, log=False),\n",
    "        'XGB__n_estimators': trial.suggest_categorical('XGB__n_estimators', [50, 70, 100, 130, 150]),\n",
    "        'XGB__eta': trial.suggest_float('XGB__eta', 0, 0.3, step=0.02, log=False),\n",
    "        'XGB__alpha': trial.suggest_float('XGB__alpha', 0.2, 1, step=0.01, log=False)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_RF_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'RF__n_estimators': trial.suggest_categorical('RF__n_estimators', [30, 50, 75, 100, 120, 150, 200, 250]),\n",
    "        'RF__max_depth': trial.suggest_categorical('RF__max_depth', [3, 4, 5, 7, 10, 20, 30]),\n",
    "        'RF__min_samples_split': trial.suggest_int('RF__min_samples_split', 2, 20),\n",
    "        'RF__min_samples_leaf': trial.suggest_int('RF__min_samples_leaf', 2, 20),\n",
    "        'RF__criterion': trial.suggest_categorical('RF__criterion', ['gini', 'entropy']),\n",
    "    })\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_linear_SVM_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LinearSVM__C': trial.suggest_float('SVM__C', 0.001, 2, log=True),\n",
    "        'LinearSVM__class_weight': trial.suggest_categorical('LinearSVM__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_MLP_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'MLP__learning_rate_init': trial.suggest_float('MLP__learning_rate_init', 0.0001, 0.2, log=True),\n",
    "        'MLP__alpha': trial.suggest_float('MLP__alpha', 0.01, 1, log=True)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_HGB_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'HGB__max_depth': trial.suggest_categorical('HGB__max_depth', [5, 10, 20, 30, 40, 50]),\n",
    "        'HGB__l2_regularization': trial.suggest_float('HGB__l2_regularization', 0.01, 0.7, log=True),\n",
    "        'HGB__max_iter': trial.suggest_categorical('HGB__max_iter', [50, 70, 100, 130, 150])\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_logistic_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'Logistic__penalty':  trial.suggest_categorical('Logistic__penalty', ['l1', 'l2', 'elasticnet', None]),\n",
    "        'Logistic__C': trial.suggest_float('Logistic__C', 0.001, 2, log=True),\n",
    "        'Logistic__class_weight': trial.suggest_categorical('Logistic__class_weight', ['balanced', None])\n",
    "    })\n",
    "\n",
    "    if param_grid['Logistic__penalty'] == 'elasticnet':\n",
    "        param_grid.update({'Logistic__l1_ratio': trial.suggest_float('Logistic__l1_ratio', 0.1, 1, log=True)})\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_SVM_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'SVM__C': trial.suggest_float('SVM__C', 0.1, 5, log=True),\n",
    "        'SVM__kernel': trial.suggest_categorical('SVM__kernel', ['poly', 'rbf', 'sigmoid']),\n",
    "    })\n",
    "\n",
    "    if param_grid['SVM__kernel'] == 'poly':\n",
    "\n",
    "        param_grid.update({\n",
    "            'SVM__degree': trial.suggest_int('SVM__degree', 1, 5)\n",
    "        })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grid_LGBM_CNN(trial):\n",
    "\n",
    "    param_grid = preprocessing_CNN_param_grid(trial)\n",
    "\n",
    "    param_grid.update({\n",
    "        'LGBM__max_depth': trial.suggest_int('LGBM__max_depth', 2, 200),\n",
    "        'LGBM__num_leaves': trial.suggest_int('LGBM__num_leaves', 2, 200),\n",
    "        'LGBM__n_estimators': trial.suggest_categorical('LGBM__n_estimators', [30, 50, 70, 100, 120, 150, 180, 200, 250, 300]),\n",
    "        'LGBM__learning_rate': trial.suggest_float('LGBM__learning_rate', 0.0001, 0.1, log=True),\n",
    "        'LGBM__lambda_l1': trial.suggest_float('LGBM__lambda_l1', 0.001, 1, log=True),\n",
    "        'LGBM__lambda_l2': trial.suggest_float('LGBM__lambda_l2', 0.001, 1, log=True),\n",
    "        'LGBM__min_split_gain': trial.suggest_float('LGBM__min_split_gain', 0.001, 0.01, log=True),\n",
    "        'LGBM__min_child_weight': trial.suggest_int('LGBM__min_child_weight', 5, 60),\n",
    "        'LGBM__feature_fraction': trial.suggest_float('LGBM__feature_fraction', 0.1, 0.9, step=0.05)\n",
    "    })\n",
    "\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyper-parameter Optimization (HPO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'knn'\n",
    "dict_name = 'knn-CNN'\n",
    "param_grid = param_grid_knn_CNN\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=5, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RF'\n",
    "dict_name = 'RF-CNN'\n",
    "param_grid = param_grid_RF_CNN\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=2, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Logistic'\n",
    "dict_name = 'Logistic-CNN'\n",
    "param_grid = param_grid_logistic_CNN\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=1, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LinearSVM'\n",
    "dict_name = 'LinearSVM-CNN'\n",
    "param_grid = param_grid_linear_SVM_CNN\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=1, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MLP'\n",
    "dict_name = 'MLP-CNN'\n",
    "param_grid = param_grid_MLP_CNN\n",
    "\n",
    "simple_eval = SimpleEvaluation(estimator=pipelines[model_name],  \n",
    "                               cv=inner, \n",
    "                               param_grid=param_grid,\n",
    "                               search_method='optuna',\n",
    "                               scoring='accuracy', \n",
    "                               direction='maximize', \n",
    "                               n_trials=1, \n",
    "                               random_state=123)\n",
    "\n",
    "simple_eval.fit(X=X_train, y=Y_train)\n",
    "\n",
    "inner_score[dict_name] = simple_eval.inner_score\n",
    "best_params[dict_name] = simple_eval.inner_best_params\n",
    "inner_results[dict_name] = simple_eval.inner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Saving the inner results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('results/inner_score.pkl', 'wb') as file:\n",
    "    pickle.dump(inner_score, file)\n",
    "with open('results/best_params.pkl', 'wb') as file:\n",
    "    pickle.dump(best_params, file)\n",
    "with open('results/inner_results.pkl', 'wb') as file:\n",
    "    pickle.dump(inner_results, file)\n",
    "'''    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
